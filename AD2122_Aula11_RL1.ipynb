{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AD2122_RL1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHlWaMq5PQz4"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "    %pip install -U tf-agents pyvirtualdisplay\n",
        "    %pip install -U gym>=0.21.0\n",
        "    %pip install -U gym[box2d,atari,accept-rom-license]\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gym.openai.com/docs/\n",
        "    \n",
        "import gym"
      ],
      "metadata": {
        "id": "2uRIbLi4PiBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o ambiente CartPole\n",
        "# https://gym.openai.com/envs/CartPole-v1/\n",
        "\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "awH5jDv5PuOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O ambiente é inicializado através da chamada ao método reset()\n",
        "# O método devolve a primeira observação, no formato de um array NumPy com 4 valores reais\n",
        "# (Posição horizontal, Velocidade, Ângulo de inclinação, Velocidade angular)\n",
        "\n",
        "\n",
        "obs = env.reset(seed=42)"
      ],
      "metadata": {
        "id": "OrwSPH_APw2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Posição horizontal, Velocidade, Ângulo de inclinação, Velocidade Angular)\n",
        "\n",
        "obs"
      ],
      "metadata": {
        "id": "UnB26CJbPzk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para garantir que o ambiente executa corretamente no Colab\n",
        "\n",
        "try:\n",
        "    import pyvirtualdisplay\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "J_e4pa2qTenX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter uma imagem do ambiente no formato NumPy Array\n",
        "# Poderá também surgir um pop-up com a imagem. Se for esse o caso, podem ignorar essa janela\n",
        "\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "id": "o7D0z5oQP2Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render(mode=\"rgb_array\")\n",
        "img.shape"
      ],
      "metadata": {
        "id": "X2XR-HdxWvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_environment(env, figsize=(5,4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img"
      ],
      "metadata": {
        "id": "XLz7Z3YOW00p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar a imagem obtida\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3GW5TLcXo4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar quantas ações é possível efetuar neste ambiente\n",
        "\n",
        "env.action_space"
      ],
      "metadata": {
        "id": "BRmNnGxxX98j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - accelerate left\n",
        "# 1 - accelerate right\n",
        "\n",
        "action = 1  "
      ],
      "metadata": {
        "id": "bNEr1DFa5VDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar uma ação\n",
        "# Devolve nova observação, recompensa, informação sobre o final do episódio e outra informação \n",
        "\n",
        "obs, reward, done, info = env.step(action)\n",
        "print('Obs: ', obs, '\\nReward: ', reward, '\\nDone: ', done, '\\nInfo:', info)"
      ],
      "metadata": {
        "id": "xkmn657X5Yuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuar a acelerar para a direita\n",
        "# Verificar se as variações nas observações são consistentes com esta ação\n",
        "# Eventualmente a simulação vai terminar\n",
        "\n",
        "for i in range(50):\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('Obs: ', obs, '\\nReward: ', reward, '\\nDone: ', done, '\\nInfo:', info)\n",
        "  if done == True:\n",
        "    break\n"
      ],
      "metadata": {
        "id": "RQFeYBW-6Chw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quando termina a simulação, é necessário reinicializar o ambiente\n",
        "\n",
        "if done:\n",
        "    obs = env.reset(seed=42)"
      ],
      "metadata": {
        "id": "kUjX-YvJ6-eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### PARTE 1 ######################################\n",
        "# Definição de Políticas Fixas\n",
        "\n",
        "# Política Reativa Simples\n",
        "# Acelerar para tentar diminuir a inclinação do poste\n",
        "# a observação obs[2] contém o ângulo de inclinação. \n",
        "\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1"
      ],
      "metadata": {
        "id": "C8neD92L7B9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simular 500 episódios \n",
        "# Recolher as recompensas em cada um destes episódios\n",
        "# Cada episódio termina quando a variável done fica a true.\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "p5HjtgdW7PW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar o desempenho médio e a duração dos episódios com maior e menor duração\n",
        "# Analise os resultados\n",
        "\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ],
      "metadata": {
        "id": "fQhs3s3p7XTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar um episódio e recolher imagens em cada um dos passos\n",
        "\n",
        "frames = []\n",
        "\n",
        "obs = env.reset(seed=42)\n",
        "for step in range(200):\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    action = basic_policy(obs)\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "uA0JvBOR7iRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "metadata": {
        "id": "pYVxL-zR7p1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar um episódio\n",
        "\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "NK-IO8Oq7s4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Politica Reativa Melhorada\n",
        "\n",
        "# Complete o método not_so_basic_policy(obs) para tentar definir uma política fixa com melhor desempenho do que a anterior\n",
        "\n",
        "import random\n",
        "\n",
        "# Para já o método está a devolver cada uma das ações com probabilidade 0.5, ignorando a observação\n",
        "# Altere o que achar conveniente para melhorar o desempenho\n",
        "\n",
        "def not_so_basic_policy(obs):\n",
        "  return random.randint(0, 1)"
      ],
      "metadata": {
        "id": "9Eo5n_l679O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular 500 episódios com a nova politica\n",
        "# Compare os resultados obtidos com os da politica básica\n",
        "\n",
        "totals = []\n",
        "\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = not_so_basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n",
        "\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ],
      "metadata": {
        "id": "d_ePVQLd8Nqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### PARTE 2 ######################################\n",
        "\n",
        "# Neural Network Policies\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "LHGaRBBN9oBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma rede neuronal básica que recebe uma observação (4 valores) e devolve a probabilidade de executar a ação 0\n",
        "\n",
        "n_inputs = 4 \n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "0js7ExCp9w3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Função que executa um episódio e recolhe as imagens para visualização numa animação\n",
        "\n",
        "def render_policy_net(model, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print('Performed', step, 'steps')\n",
        "            break\n",
        "    env.close()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "XGAZ7048-acl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho da rede\n",
        "# Ela ainda não foi treinada e terá um desempenho aleatório\n",
        "\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "BOif0Rl1-fRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aprender uma estratégia básica -> Problema resolvido como uma situação supervisionada\n",
        "# Pretende-se que a rede aprenda uma estratégia igual à estratégia básica que implementámos anteriormente\n",
        "# Neste caso o target será definido em função do ângulo de inclinação do poste (obs[2]).\n",
        "\n",
        "# Treinar em 50 ambientes diferentes, durante 5000 iterações\n",
        "\n",
        "n_environments = 50\n",
        "n_iterations = 5000\n",
        "\n",
        "envs = [gym.make(\"CartPole-v1\") for _ in range(n_environments)]\n",
        "for index, env in enumerate(envs):\n",
        "    env.seed(index)\n",
        "np.random.seed(42)\n",
        "observations = [env.reset() for env in envs]\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Define the supervised target -> if angle < 0, we want proba(left) = 1., or else proba(left) = 0.  \n",
        "    target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
        "                              for obs in observations])\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_probas = model(np.array(observations))\n",
        "        loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
        "    print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
        "    for env_index, env in enumerate(envs):\n",
        "        obs, reward, done, info = env.step(actions[env_index][0])\n",
        "        observations[env_index] = obs if not done else env.reset()\n",
        "\n",
        "for env in envs:\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "4kGF_1Ry-5AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho do modelo treinado\n",
        "# Mudar a seed s para testar o comportamento em ambientes diferentes\n",
        "\n",
        "s = 10\n",
        "\n",
        "frames = render_policy_net(model, 200, s)\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "H_CCt2KX_M8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementar uma política baseada em gradientes, utilizando uma variante do método REINFORCE\n",
        "\n",
        "# Método auxiliar para executar um passo de um episódio\n",
        "# Guarda os gradientes e a loss, mas não os aplica\n",
        "\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "\n",
        "# Método que permite executar vários episódios e guardar os gradientes e as recompensas\n",
        "\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset(seed=42)\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads\n",
        "\n",
        "# Métodos auxiliares para processamento de recompensas\n",
        "\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_rate\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]\n"
      ],
      "metadata": {
        "id": "U-XDifee_fRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar e preparar a rede para o treino com o algoritmo indicado\n",
        "\n",
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_rate = 0.95\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[4]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "_2SSVI_E_riQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar\n",
        "# Verificar como a recompensa média vai aumentando ao longo das iterações de treino\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    total_rewards = sum(map(sum, all_rewards))                     \n",
        "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          \n",
        "        iteration, total_rewards / n_episodes_per_update), end=\"\") \n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_rate)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "cnN-yEMD_zKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho do modelo treinado com o algoritmo REINFORCE\n",
        "# Mudar a seed s para testar o comportamento em ambientes diferentes\n",
        "\n",
        "s = 41\n",
        "\n",
        "frames = render_policy_net(model, 200, s)\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "wpbeYUsUDpMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}