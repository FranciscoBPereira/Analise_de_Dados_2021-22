{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AD2122_RL2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdYSVhhDW5XU"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "    %pip install -U tf-agents pyvirtualdisplay\n",
        "    %pip install -U gym>=0.21.0\n",
        "    %pip install -U gym[box2d,atari,accept-rom-license]\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o ambiente Cart Pole\n",
        "# Definir a rede neuronal\n",
        "\n",
        "import gym\n",
        "from tensorflow import keras\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "\n",
        "input_shape = [4] # == env.observation_space.shape\n",
        "n_outputs = 2 # == env.action_space.n\n",
        "\n",
        "# Rede neuronal com 2 camadas escondidas (32 nós cada)\n",
        "# Input - Estado\n",
        "# Output - Q-value para cada ação\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "])"
      ],
      "metadata": {
        "id": "FHGc9nwxXWv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para garantir que o ambiente executa corretamente no Colab\n",
        "\n",
        "try:\n",
        "    import pyvirtualdisplay\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "I0v95S4NYcJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para garantir que o agente  mantém capacidade de exploração, ele poderá escolher \n",
        "# uma ação aleatória com probabilidade epsilon (hiper-parâmetro do algoritmo DQN)\n",
        "# Nesta implementação é um parâmetro adaptativo que diminui linearmente ao longo do tempo\n",
        "\n",
        "# Caso contrário, escolhe a ação com maior Q-value \n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])"
      ],
      "metadata": {
        "id": "oQismx3cZbnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções auxiliares para treino\n",
        "\n",
        "# É criada uma memória de replay para alimentar os exemplos usados durante o treino (experience replay ou replay buffer)\n",
        "# O método sample_experiences efetua a amostragem aleatória da memória de replay para criar um batch de treino\n",
        "\n",
        "# Cada experiência tem 5 componentes: estado, ação executada, recompensa recebida, novo estado, final de episódio (true/false)\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "replay_memory = deque(maxlen=2000)\n",
        "\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
        "    batch = [replay_memory[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "metadata": {
        "id": "TG4asbRoZfcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Este método efetua uma ação e recolhe o resultado\n",
        "# A informação é adicionada à memória de replay\n",
        "\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_memory.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "TnXCwgDdZh71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação de um batch de experiências para aplicação de uma iteração de treino da DQN\n",
        "\n",
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr=1e-2)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "# Passos deste método\n",
        "# 1. Obter um batch de exemplos da memória de replay\n",
        "# 2. Aplicar a DQN para obter a previsão dos Q-values\n",
        "# 3. Calcular o valor dos targets\n",
        "# 4. Calcular loss\n",
        "# 5. Aplicar SGD\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "zfdHKMAzZldS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rewards = [] \n",
        "best_score = 0"
      ],
      "metadata": {
        "id": "936vuKvEZoT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo durante 500 episódios\n",
        "\n",
        "# Os primeiros 50 episódios servem apenas alimentar a memória de replay\n",
        "\n",
        "for episode in range(500):\n",
        "    obs = env.reset()    \n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(step) \n",
        "    if step >= best_score: \n",
        "        best_weights = model.get_weights() \n",
        "        best_score = step \n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "\n",
        "model.set_weights(best_weights)"
      ],
      "metadata": {
        "id": "7HyYqxEmZvR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico que mostra a evolução das recomoenpensas recebidas ao longo dos episódios de treino\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\", fontsize=14)\n",
        "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QQi1hm13bzo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "metadata": {
        "id": "wIBMec7Vb2QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar o comportamento da estratégia que resulta do treino\n",
        "\n",
        "state = env.reset(seed=42)\n",
        "\n",
        "frames = []\n",
        "\n",
        "for step in range(200):\n",
        "    action = epsilon_greedy_policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    \n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "sgUhJbZJb6Ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}